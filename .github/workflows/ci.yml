name: CI

on:
  push:
  pull_request:

jobs:
  test:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Restore pip cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install PyTorch CPU
        run: |
          python -m pip install --upgrade pip
          pip install --index-url https://download.pytorch.org/whl/cpu \
            torch torchvision torchaudio

      - name: Install project deps
        run: |
          pip install -r requirements.txt --no-deps
          # tensorboard deps required by torch.utils.tensorboard
          pip install google protobuf absl-py werkzeug markdown antlr4-python3-runtime pyyaml
          # dev tools for lint/type/test
          pip install ruff mypy pytest
          pip install -e .

      - name: Sanity check
        env:
          PYTHONPATH: .
        run: |
          python examples/sanity_check.py
          lstc sanity

      - name: Lint (ruff)
        run: ruff check .

      - name: Type check (mypy)
        run: mypy .

      - name: Unit tests (pytest)
        env:
          PYTHONPATH: .
        run: pytest -q

      - name: Generate toy datasets (single & multiview)
        env:
          PYTHONPATH: .
        run: |
          python examples/gen_toy_dataset.py --out ./toy_data --subjects 3 --seq-per-subject 2 --frames 8
          python examples/gen_toy_dataset.py --out ./toy_view0 --subjects 3 --seq-per-subject 2 --frames 8 --seed 101
          python examples/gen_toy_dataset.py --out ./toy_view1 --subjects 3 --seq-per-subject 2 --frames 8 --seed 202

      - name: Train CE (toy, 1 epoch, CPU)
        env:
          PYTHONPATH: .
        run: |
          python examples/train_real.py --data-root ./toy_data --epochs 1 --batch-size 8 --seq-len 8 --device cpu

      - name: Train CE+Triplet (toy, 1 epoch, CPU)
        env:
          PYTHONPATH: .
        run: |
          python examples/train_metric.py --data-root ./toy_data --epochs 1 --batch-p 2 --batch-k 2 --seq-len 8 --device cpu

      - name: Train CE (multiview, 1 epoch, CPU)
        env:
          PYTHONPATH: .
        run: |
          cat > /tmp/mv.yaml <<'YAML'
          seed: 42
          amp: false
          data_roots: ["./toy_view0", "./toy_view1"]
          seq_len: 8
          height: 64
          width: 44
          num_workers: 0
          split_ratio: 0.8
          in_channels: 1
          base_channels: 16
          num_stripes: 8
          embedding_dim: 128
          epochs: 1
          batch_size: 8
          lr: 3e-4
          weight_decay: 0.05
          out_dir: runs/lstc_real_mv
          log_dir: runs/logs_real_mv
          tensorboard: false
          csv_log: false
          YAML
          python examples/train_real_multiview.py --config /tmp/mv.yaml

      - name: Cross-view eval (self-gallery)
        env:
          PYTHONPATH: .
        run: |
          python examples/eval_retrieval_multiview.py --config /tmp/mv.yaml --ckpt runs/lstc_real_mv/best.pt

      - name: Execute quick_start_safe notebook (smoke)
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: |
          pip install -q nbconvert nbformat jupyter grpcio tensorboard-data-server
          python - <<'PY'
import nbformat as nbf
from pathlib import Path
nb = nbf.read(Path('notebooks/quick_start_safe.ipynb'), as_version=nbf.NO_CONVERT)
nbf.validate(nb)  # triggers warnings; normalization step below adds missing ids
changed = False
for cell in nb.cells:
    if 'id' not in cell:
        cell['id'] = nbf.v4.new_id()  # type: ignore[attr-defined]
        changed = True
if changed:
    nbf.write(nb, Path('notebooks/quick_start_safe.ipynb'))
print('Notebook IDs normalized')
PY
          jupyter nbconvert --to notebook --execute notebooks/quick_start_safe.ipynb \
            --output /tmp/quick_start_safe.out.ipynb --ExecutePreprocessor.timeout=600

      - name: Export TorchScript and check equivalence
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: |
          # Export TorchScript from the CE checkpoint
          lstc export --ckpt runs/lstc_real/best.pt --seq-len 8 --height 64 --width 44 --torchscript-out runs/export/model.ts
          python - <<'PY'
import torch
from pathlib import Path
from lstc import LSTCBackbone

ckpt = torch.load('runs/lstc_real/best.pt', map_location='cpu')
embed_dim = ckpt.get('args', {}).get('embedding_dim', 256)
num_stripes = ckpt.get('args', {}).get('num_stripes', 8)
base_channels = ckpt.get('args', {}).get('base_channels', 16)
model = LSTCBackbone(in_channels=1, base_channels=base_channels, num_stripes=num_stripes, embedding_dim=embed_dim)
model.load_state_dict(ckpt['model'])
model.eval()

ts = torch.jit.load('runs/export/model.ts')
ts.eval()

x = torch.randn(1, 1, 8, 64, 44)
with torch.no_grad():
    e_ref = model(x)['embedding']
    e_ts = ts(x)

def l2n(t):
    return t / (t.norm(dim=1, keepdim=True) + 1e-12)
e_ref_n = l2n(e_ref)
e_ts_n = l2n(e_ts)
cos = torch.sum(e_ref_n * e_ts_n, dim=1).mean().item()
print({'cosine': cos})
assert cos > 0.999, f'Cosine similarity too low: {cos}'
PY
